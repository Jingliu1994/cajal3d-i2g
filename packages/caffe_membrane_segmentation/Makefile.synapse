#-------------------------------------------------------------------------------
# Synapse Classification Experiment
#
#  As of this writing, there are two training files. Y_train.mat is for the
#  synapse-vs-all problem while Y_train2.mat is for the
#  synapse-membranes-vs-non-synapse-membranes problem.
#
#     Y_train.mat  :  Held out pixels := -1
#                     Non-synapse pixels (membrane & non-membrane) := 0
#                     Synapse pixels := +1,
#
#     Y_train2.mat :  Held out pixels (includes most non-membrane data) := -1
#                     Non-synapse membranes := 0
#                     Synapse pixels := +1, 
#
# April 2015
#-------------------------------------------------------------------------------


include Makefile

SYN_OUT_DIR := SynapseDetection
SYN_IN_DIR := synapse_isbi2013

TRAIN_SLICES := "range(0,70)"
VALID_SLICES := "range(70,100)"
ALL_SLICES := "range(100)"

Y_TRAIN := Y_train2.mat

# set ROTATE to {0,1} for {fewer,more} rotations of training data
ROTATE := 1

# the trained model to use for deploy targets
MODEL := $(SYN_OUT_DIR)/iter_080000.caffemodel


#-------------------------------------------------------------------------------

train-all:
	PYTHONPATH=$(CAFFE_DIR) nohup python train.py  \
		-X $(SYN_IN_DIR)/X_train.mat \
		-Y $(SYN_IN_DIR)/$(Y_TRAIN)  \
		--train-slices $(ALL_SLICES) \
		--rotate-data $(ROTATE) \
		--snapshot-prefix $(SYN_OUT_DIR) \
		-s caffe_files/n3-solver.prototxt \
		--omit-labels "[-1,]" \
		-gpu 1 &> nohup.train.all.synapse &


train-and-valid:
	PYTHONPATH=$(CAFFE_DIR) nohup python train.py  \
		-X $(SYN_IN_DIR)/X_train.mat \
		-Y $(SYN_IN_DIR)/$(Y_TRAIN) \
		--train-slices $(TRAIN_SLICES) \
		--valid-slices $(VALID_SLICES) \
		--rotate-data $(ROTATE) \
		--snapshot-prefix $(SYN_OUT_DIR) \
		-s caffe_files/n3-solver.prototxt \
		--omit-labels "[-1,]" \
		-gpu 2 &> nohup.train.and.valid.synapse &


# Technically we don't need to evaluate the entire training cube, just
# the subset we hold out for validation.  However, for some purposes
# it is convenient to assess the performance on the entire training
# data set, so we do this here.
deploy-valid:
	PYTHONPATH=$(CAFFE_DIR) nohup python deploy.py  \
		-X $(SYN_IN_DIR)/X_train.mat \
		-s caffe_files/n3-solver.prototxt \
		-m $(MODEL) \
		--eval-slices $(ALL_SLICES) \
		--yhat-file $(SYN_OUT_DIR)/Yhat_train \
		-gpu 5 &> nohup.deploy.train.synapse &


deploy-test:
	PYTHONPATH=$(CAFFE_DIR) nohup python deploy.py  \
		-X $(SYN_IN_DIR)/X_test.mat \
		-s caffe_files/n3-solver.prototxt \
		-m $(MODEL) \
		--yhat-file $(SYN_OUT_DIR)/Yhat_test \
		-gpu 4 &> nohup.deploy.synapse &


# These next few targets are for feature extraction
extract-train:
	PYTHONPATH=$(CAFFE_DIR) nohup python deploy.py  \
		-X $(SYN_IN_DIR)/X_train.mat \
		-s caffe_files/n3-solver.prototxt \
		-m $(MODEL) \
		--yhat-file $(SYN_OUT_DIR)/Yhat_train \
		--feature-file $(SYN_OUT_DIR)/Xprime_train \
		-gpu 3 &> nohup.extract.train.synapse  &

extract-test:
	PYTHONPATH=$(CAFFE_DIR) nohup python deploy.py  \
		-X $(SYN_IN_DIR)/X_test.mat \
		-s caffe_files/n3-solver.prototxt \
		-m $(MODEL) \
		--yhat-file $(SYN_OUT_DIR)/Yhat_test \
		--feature-file $(SYN_OUT_DIR)/Xprime_test \
		-gpu 4 &> nohup.extract.test.synapse &


tar-synapse : tar
	cp $(DATA_PATH)/SynapseISBI2013/*.mat $(PACKAGE_PATH)/caffe_files/$(SYN_IN_DIR)
	pushd .. && $(TAR) rvf $(PACKAGE_NAME)/tocluster.tar `find $(PACKAGE_NAME) -name \*.mat -print`
